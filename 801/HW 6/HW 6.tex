\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,bm,fullpage,array}

\newcommand{\x}{\bm{x}}
\renewcommand{\a}{\bm{a}}
\renewcommand{\b}{\bm{b}}
\renewcommand{\u}{\bm{u}}
\renewcommand{\v}{\bm{v}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\zero}{\bm{0}}

\title{Chase Joyner}
\author{801 Homework 6}
\date{November 24, 2015}

\begin{document}
\maketitle

\section*{Problem 1:}  A scale has two pans.  The measurement given by the scale is the difference between the weights in pan $\# 1$ and pan $\# 2$ plus a random error.  Thus, if a weight $\mu_1$ is put in pan $\# 1$, a weight $\mu_2$ is put in pan $\# 2$, then the measurement is $Y = \mu_1 - \mu_2 + \epsilon$.  Suppose that E$(\epsilon) = 0$, Var$(\epsilon) = \sigma^2$, and that in repeated uses of the scale, observations $Y_i$ are independent. \\
Suppose that two objects, $\# 1$ and $\# 2$, have weights $\beta_1$ and $\beta_2$.  Measurements are then taken as follows:
\begin{enumerate}
\item Object $\# 1$ is put on pan $\# 1$, nothing on pan $\# 2$. \vspace{-7mm}\\
\item Object $\# 2$ is put on pan $\# 1$, nothing on pan $\# 2$. \vspace{-7mm}\\
\item Object $\# 1$ is put on pan $\# 1$, object $\# 2$ on pan $\# 2$. \vspace{-7mm}\\
\item Objects $\# 1$ and $\# 2$ are both put on pan $\# 1$.
\end{enumerate}
\begin{itemize}
\item[(a)]  Let ${\bf Y} = (Y_1,Y_2,Y_3,Y_4)'$ be the vector of observations.  Formulate this as a linear model.
\item[(b)]  Find vectors ${\bf a}_1$ and ${\bf a}_2$ such that $\widehat{\beta}_1 = {\bf a}_1^T{\bf Y}$ and $\widehat{\beta}_2 = {\bf a}_2^T{\bf Y}$ are the least squares estimators of $\beta_1$ and $\beta_2$. 
\item[(c)] Find the covariance matrix for $\widehat{\beta} = (\widehat{\beta}_1,\widehat{\beta}_2)'$.
\item[(d)] Find a matrix ${\bf A}$ such that $S^2 = {\bf Y}'{\bf A}{\bf Y}$.
\item[(e)]  For the observation ${\bf Y} = (7,3,1,7)'$, find $S^2$, and estimate the covariance matrix of $\widehat{\beta}$.
\item[(f)]  Show that four such weighings can be made in such a way that the least squares estimators of $\beta_1$ and $\beta_2$ have smaller variances than for the experiment above.
\end{itemize}
\begin{itemize}
\item[] {\bf Solution:}
\begin{itemize}
\item[(a)]  The linear model for this experiment can be written as
\[
Y = \begin{bmatrix}
1 & 0 \\
0 & 1 \\
1 & -1 \\
1 & 1
\end{bmatrix} \begin{bmatrix}
\beta_1 \\ \beta_2
\end{bmatrix} + \begin{bmatrix}
\epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \epsilon_4
\end{bmatrix}.
\]
\item[(b)]  Recall that the least squares estimator for $\beta$ is $\widehat{\beta} = (X'X)^-X'Y$.  So, it follows that
\[
a = (X'X)^-X' = \begin{bmatrix}
\frac{1}{3} & 0 & \frac{1}{3} & \frac{1}{3} \\
0 & \frac{1}{3} & -\frac{1}{3} & \frac{1}{3}
\end{bmatrix}.
\]
Therefore, $a_1' = (1/3, 0, 1/3, 1/3)$ and $a_2' = (0, 1/3, -1/3, 1/3)$.
\item[(c)]  By part (b), we have $\widehat{\beta} = a^TY$.  Therefore, we have
\[
\text{Cov}(\widehat{\beta}) = \text{Cov}(a^TY) = a^T\text{Cov(Y)}a = \sigma^2a^Ta = \sigma^2(X'X)^- = \sigma^2\begin{bmatrix}
\frac{1}{3} & 0 \\
0 & \frac{1}{3}
\end{bmatrix}.
\]
\item[(d)] Recall that an estimate of $\sigma^2$ is
\[
S^2 = MSE = \frac{Y'(I-M)Y}{r(I-M)}.
\]
It follows that $A$ is the matrix
\[
A = \frac{I-M}{r(I-M)}.
\]
Note that
\[
M = X(X'X)^-X' = \begin{bmatrix}
\frac{1}{3} & 0 & \frac{1}{3} \vspace{1.5mm}& \frac{1}{3} \\
0 & \frac{1}{3} & -\frac{1}{3} \vspace{1.5mm}& \frac{1}{3} \\
\frac{1}{3} &  -\frac{1}{3} \vspace{1.5mm}& \frac{2}{3} & 0 \\
\frac{1}{3} & \frac{1}{3} \vspace{1.5mm}& 0 & \frac{2}{3}
\end{bmatrix}.
\]
Therefore, we have
\[
I - M = \begin{bmatrix}
\frac{2}{3} & 0 & -\frac{1}{3} \vspace{1.5mm} & -\frac{1}{3} \\
0 & \frac{2}{3} & \frac{1}{3} \vspace{1.5mm} & -\frac{1}{3} \\
-\frac{1}{3} & \frac{1}{3} \vspace{1.5mm} & \frac{1}{3} & 0 \\
-\frac{1}{3} & -\frac{1}{3} & 0 & \frac{1}{3}
\end{bmatrix}.
\]
Then, notice $r(I-M) = r(I) - r(M) = 4 - 2 = 2$, and so
\[
A = \frac{I-M}{r(I-M)} = \begin{bmatrix}
\frac{1}{3} & 0 & -\frac{1}{6} \vspace{1.5mm}& -\frac{1}{6} \\
0 & \frac{1}{3} & \frac{1}{6} \vspace{1.5mm}& -\frac{1}{6} \\
-\frac{1}{6} & \frac{1}{6} & \frac{1}{6} \vspace{1.5mm}& 0 \\
-\frac{1}{6} \vspace{1.5mm}& -\frac{1}{6} & 0 & \frac{1}{6}
\end{bmatrix}.
\]
\item[(e)]  By part (d), we have
\[
S^2 = Y'AY = \begin{bmatrix}
7 & 3 & 1 & 7
\end{bmatrix}\begin{bmatrix}
\frac{1}{3} & 0 & -\frac{1}{6} \vspace{1.5mm}& -\frac{1}{6} \\
0 & \frac{1}{3} & \frac{1}{6} \vspace{1.5mm}& -\frac{1}{6} \\
-\frac{1}{6} & \frac{1}{6} & \frac{1}{6} \vspace{1.5mm}& 0 \\
-\frac{1}{6} \vspace{1.5mm}& -\frac{1}{6} & 0 & \frac{1}{6}
\end{bmatrix}\begin{bmatrix}
7 \\ 3 \\ 1 \\ 7
\end{bmatrix} = 3.
\]
Also, by part (b), an estimate of Cov$(\widehat{\beta})$ is
\[
\widehat{\text{Cov}(\widehat{\beta})} = \widehat{\sigma}^2\begin{bmatrix}
\frac{1}{3} & 0 \\
0 & \frac{1}{3}
\end{bmatrix} = \begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}.
\]
\item[(f)]  We can change the experiment to give design matrix
\[
X = \begin{bmatrix}
1 & -1 \\
1 & 1 \\
1 & -1 \\
1 & 1
\end{bmatrix}.
\]
Then, we calculate
\[
\text{Cov}(\widehat{\beta}) = \sigma^2(X'X)^- = \sigma^2\begin{bmatrix}
\frac{1}{4} & 0 \\
0 & \frac{1}{4}
\end{bmatrix}.
\]
Since the diagonal of this matrix is smaller than before, we conclude this experiment gives smaller variances for the LSE of $\beta_1$ and $\beta_2$.
\end{itemize}
\end{itemize}

\section*{Problem 2:}  Consider the weighing problem above with the two unknown weights $\beta_1$ and $\beta_2$.
\begin{itemize}
\item[(a)]  Suppose we want Scheffe 95$\%$ confidence intervals on all linear combinations of $\beta_1$ and $\beta_2$.  For the four weighings made and for ${\bf Y} = (7,3,1,7)'$, find these intervals for the three linear combinations $\beta_1$, $\beta_2$, and $\beta_1 - \beta_2$. 
\item[(b)] Use the Bonferroni method to find 95$\%$ simultaneous confidence intervals on these same three linear combinations.
\item[(c)]  Suppose that we wish to test $H_0\colon \beta_1 = \beta_2 = 0$.  Then, $\sup_{{\bf c}\in C} t^2_c / q = F$ is the corresponding $F$--statistic.  For which value of ${\bf c}$ does $t^2_c/q = F$?  What is the corresponding ${\bf a}_c?$
\item[(d)]  Find a 95$\%$ confidence ellipsoid on $\boldsymbol\beta = (\beta_1,\beta_2)'$ for these data.
\end{itemize}
\begin{itemize}
\item[] {\bf Solution:}
\begin{itemize}
\item[(a)]  Recall that a $100(1-\alpha)\%$ Scheffe confidence intervals for all linear functions of $\beta$ are given by
\[
\left[c^T\widehat{\beta}\pm \sqrt{p\cdot MSE\cdot f_{\alpha,p,n-p}\cdot c^T(X^TX)^{-1}c} \right].
\]
We found $MSE = 3$ in part (e) of problem 1.  Note that the design matrix is
\[
X = \begin{bmatrix}
1 & 0 \\
0 & 1 \\
1 & -1 \\
1 & 1
\end{bmatrix}
\]
and so $p = r(X) = 2$.  If $c_1 = (1,0)'$, $c_2 = (0,1)'$, and $c_3 = (1,-1)'$, then
\[
c_1'\beta = \beta_1 \hspace{5mm} c_2'\beta = \beta_2 \hspace{5mm} c_3'\beta = \beta_1-\beta_2.
\]
Lastly, by part (a) from problem 1, 
\[
\widehat{\beta} = \begin{bmatrix}
\frac{1}{3} & 0 & \frac{1}{3} & \frac{1}{3} \\
0 & \frac{1}{3} & -\frac{1}{3} & \frac{1}{3}
\end{bmatrix}Y = \begin{bmatrix}
5 \\ 3
\end{bmatrix}.
\]
Therefore, the intervals for the three linear combinations are
\begin{align*}
&\left[c_1'\widehat{\beta} \pm \sqrt{2\cdot 3 \cdot f_{0.05, 2, 4 - 2}\cdot c_1'(X'X)^{-1}c_1} \right] = \left[ -1.164, 11.164\right], \\
&\left[c_2'\widehat{\beta} \pm \sqrt{2\cdot 3 \cdot f_{0.05, 2, 4 - 2}\cdot c_2'(X'X)^{-1}c_2} \right] = \left[ -3.164, 9.164\right], \\
&\left[c_3'\widehat{\beta} \pm \sqrt{2\cdot 3 \cdot f_{0.05, 2, 4 - 2}\cdot c_3'(X'X)^{-1}c_3} \right] = \left[ -6.718, 10.718\right].
\end{align*}
\item[(b)]  Recall that a $100(1-\alpha_i)\%$ Bonferroni confidence interval is
\[
\left[ c_i'\widehat{\beta}\pm t_{\alpha_i/2, n-p}\sqrt{d_i\cdot MSE} \right],
\]
where $d_i = c_i'(X'X)^{-1}c_i$.  Note that $MSE = 3$.  Let $\alpha_1 = \alpha_2 = \alpha_3 = \alpha / 3$.  Using the same $c_1,c_2,$ and $c_3$ as in part (a), we have
\begin{align*}
d_1 &= c_1'(X'X)^{-1}c_1 = \frac{1}{3} \\
d_2 &= c_2'(X'X)^{-1}c_2 = \frac{1}{3} \\
d_3 &= c_3'(X'X)^{-1}c_3 = \frac{2}{3}.
\end{align*}
Then, the Bonferroni confidence intervals for the three linear combinations are
\begin{align*}
&\left[c_1'\widehat{\beta}\pm t_{\alpha_1/2, 4 - 2}\sqrt{d_1\cdot MSE} \right] = \left[-2.649, 12.649 \right] \\
&\left[c_2'\widehat{\beta}\pm t_{\alpha_2/2, 4 - 2}\sqrt{d_2\cdot MSE} \right] = \left[-4.649, 10.649 \right] \\
&\left[c_3'\widehat{\beta}\pm t_{\alpha_3/2, 4 - 2}\sqrt{d_3\cdot MSE} \right] = \left[-8.817, 12.817 \right].
\end{align*}
\item[(c)]  Note that under $H_0$, $X_0 = 0_{4\times 2}$ and $M_0 = 0_{4\times 4}$.  First, recall
\begin{align*}
F &= \frac{Y'(M-M_0)Y/r(M-M_0)}{Y'(I-M)Y/r(I-M)} = \frac{Y'MY/r(M)}{MSE}.
\end{align*}
We see that
\[
Y'MY = \begin{bmatrix}
7 & 3 & 1 & 7
\end{bmatrix}\begin{bmatrix}
\frac{1}{3} & 0 & \frac{1}{3} \vspace{1.5mm}& \frac{1}{3} \\
0 & \frac{1}{3} & -\frac{1}{3} \vspace{1.5mm}& \frac{1}{3} \\
0 & -\frac{1}{3} & \frac{2}{3} \vspace{1.5mm}& 0 \\
\frac{1}{3} & \frac{1}{3} & 0 \vspace{1.5mm}& \frac{2}{3}
\end{bmatrix} \begin{bmatrix}
7 \\ 3 \\ 1 \\ 7
\end{bmatrix} = 102.
\]
Also, $r(M) = 2$ and $MSE = 3$.  Therefore, $F = 17$.  Again, $r(X) = q = 2$ and so we want to find a $c$ such that $t^2_c = 34$.  Note that
\[
t_c = \frac{c'(\widehat{\beta} - \beta)}{\sqrt{MSE\cdot c'(X'X)^{-1}c}},
\]
and since $\beta = (0,0)'$, $MSE = 3$, and squaring both sides, we have
\[
t_c^2 = \frac{(5c_1 + 3c_2)^2}{c_1^2 + c_2^2}.
\]
Setting this equal to 34 and solving for $c_1$ and $c_2$, we have $c = (15,9)'$ is a solution.  Then, the corresponding $a_c$ is
\[
a_c = X(X'X)^{-1}c = \begin{bmatrix}
\frac{1}{3} & 0 \\
0 & \frac{1}{3} \\
\frac{1}{3} & -\frac{1}{3} \\
\frac{1}{3} & \frac{1}{3}
\end{bmatrix} \begin{bmatrix}
c_1 \\ c_2
\end{bmatrix} = \begin{bmatrix}
5 \\ 3 \\ 2 \\ 8
\end{bmatrix}.
\]
\item[(d)]  Recall that from our notes
\[
\frac{W^\star}{q} \sim F(q,n-p),
\]
where $W^\star = \sup_{c\in C} t_c^2 = \frac{(\widehat{\beta} - \beta)'(X'X)(\widehat{\beta} - \beta)}{S^2_c}$.  From this, we see that a $95\%$ confidence ellipsoid on $\beta$ is
\[
(\widehat{\beta} - \beta)'(X'X)(\widehat{\beta} - \beta) \leq S^2_c\cdot q \cdot f(\alpha, q, n-p).
\]
We calculate
\begin{align*}
(\widehat{\beta} - \beta)'(X'X)(\widehat{\beta} - \beta) = \left(\begin{bmatrix}
5 \\ 3
\end{bmatrix} - \begin{bmatrix}
\beta_1 \\ \beta_2
\end{bmatrix}\right)'\begin{bmatrix}
3 & 0 \\
0 & 3
\end{bmatrix}\left(\begin{bmatrix}
5 \\ 3
\end{bmatrix} - \begin{bmatrix}
\beta_1 \\ \beta_2
\end{bmatrix}\right) \leq 3\cdot 2 \cdot 19
\end{align*}
and so we see that
\[
(5 - \beta_1)^2 + (3 - \beta_2)^2 \leq 38.
\]
Therefore, a $95\%$ confidence ellipsoid on $\beta$ is given by
\[
\{\beta \mid (5 - \beta_1)^2 + (3 - \beta_2)^2 \leq 38 \}.
\]
\end{itemize}
\end{itemize}

\end{document}
